{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS0yeeMXz6yOrKxPOZQ4fs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomasonjo/blogs/blob/master/llm/openaifunction_constructing_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cllpFMqCUaA9",
        "outputId": "7e3e9d02-8bb0-430f-edd2-3bcdec0b2496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.314)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.13.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.43)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.3.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain neo4j openai wikipedia tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.graphs.graph_document import (\n",
        "    Node as BaseNode,\n",
        "    Relationship as BaseRelationship,\n",
        "    GraphDocument,\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from typing import List, Dict, Any, Optional\n",
        "from langchain.pydantic_v1 import Field, BaseModel\n",
        "\n",
        "\n",
        "class Node(BaseNode):\n",
        "    properties: Optional[str] = Field(None, description=\"List of node properties\")\n",
        "\n",
        "class Relationship(BaseRelationship):\n",
        "    properties: Optional[str] = Field(\n",
        "        None, description=\"List of relationship properties\"\n",
        "    )\n",
        "\n",
        "class KnowledgeGraph(BaseModel):\n",
        "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
        "    nodes: List[Node] = Field(\n",
        "        ..., description=\"List of nodes in the knowledge graph\")\n",
        "    rels: List[Relationship] = Field(\n",
        "        ..., description=\"List of relationships in the knowledge graph\"\n",
        "    )"
      ],
      "metadata": {
        "id": "K589QK8WUcPo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_property_key(s: str) -> str:\n",
        "    words = s.split()\n",
        "    if not words:\n",
        "        return s\n",
        "    first_word = words[0].lower()\n",
        "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
        "    return \"\".join([first_word] + capitalized_words)\n",
        "\n",
        "def string_to_dict(s: str) -> dict:\n",
        "    \"\"\"Convert a string of properties to a dictionary.\"\"\"\n",
        "    items = s.split(\"; \")\n",
        "    properties = {}\n",
        "    try:\n",
        "        for item in items:\n",
        "            key, value = item.split(\": \")\n",
        "            properties[format_property_key(key)] = value.capitalize()\n",
        "    except ValueError:  # GPT-3.5 doesn't always follow syntax\n",
        "        pass\n",
        "    return properties\n",
        "\n",
        "def map_to_base_node(node: Node) -> BaseNode:\n",
        "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
        "    properties = string_to_dict(node.properties) if node.properties else {}\n",
        "    # Add name property for better Cypher statement generation\n",
        "    properties[\"name\"] = node.id.title()\n",
        "    return BaseNode(\n",
        "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
        "    )\n",
        "\n",
        "\n",
        "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
        "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
        "    source = map_to_base_node(rel.source)\n",
        "    target = map_to_base_node(rel.target)\n",
        "    properties = string_to_dict(rel.properties) if rel.properties else {}\n",
        "    return BaseRelationship(\n",
        "        source=source, target=target, type=rel.type, properties=properties\n",
        "    )"
      ],
      "metadata": {
        "id": "-bV8nHwimqS7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chains.openai_functions import (\n",
        "    create_openai_fn_chain,\n",
        "    create_structured_output_chain,\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "def get_extraction_chain(\n",
        "    allowed_nodes: Optional[List[str]] = None,\n",
        "    allowed_rels: Optional[List[str]] = None\n",
        "    ):\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [(\n",
        "          \"system\",\n",
        "          f\"\"\"You are a world class algorithm for extracting information in structured formats to construct a knowledge graph.\n",
        "          Nodes are used to represent entities and concepts similar to Wikipedia nodes.\n",
        "          When labeling nodes in the knowledge graph, please ensure that you consistently use elementary or basic types for node labels.\n",
        "          For instance, if you encounter an entity that represents a person, always label it as \"person\" rather\n",
        "          than using more specific terms like \"mathematician\" or \"scientist.\"\n",
        "          The goal is to keep node labels at a fundamental level to maintain clarity and simplicity in the knowledge graph.\n",
        "          By doing so, we ensure that the graph remains accessible and understandable for a wide range of users.\n",
        "          {'Allowed node labels are:' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
        "          {'Allowed relationship types are:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
        "          Whenever you encounter age information or numerical data related to entities, ensure they are incorporated\n",
        "          as attributes or properties of the respective nodes.\n",
        "          Do not create separate nodes for dates or numbers, but always attach them as node attributes or properties.\n",
        "          The properties should be given in a key: value structure. Example:\n",
        "          properties: \"age: 16; height: 175cm; birthDate: 1988-05-08\"\n",
        "          Never use escaped single or double quotes inside of properties value.\n",
        "          Use camelCase format for property keys (birthDate).\n",
        "          You must follow the rules or you will be terminated.\n",
        "          \"\"\"),\n",
        "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
        "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
        "        ])\n",
        "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
      ],
      "metadata": {
        "id": "kpGkdMyuUmAn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.graphs import Neo4jGraph\n",
        "\n",
        "url = \"neo4j+s://databases.neo4j.io\"\n",
        "username =\"neo4j\"\n",
        "password = \"password\"\n",
        "graph = Neo4jGraph(\n",
        "    url=url,\n",
        "    username=username,\n",
        "    password=password\n",
        ")"
      ],
      "metadata": {
        "id": "F25P-N_DmEBK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_store_graph(document: Document, nodes:Optional[List[str]] = None, rels:Optional[List[str]]=None) -> None:\n",
        "    # Extract graph data using OpenAI functions\n",
        "    extract_chain = get_extraction_chain(nodes, rels)\n",
        "    data = extract_chain.run(document.page_content)\n",
        "    # Construct a graph document\n",
        "    graph_document = GraphDocument(\n",
        "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
        "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
        "      source = document\n",
        "    )\n",
        "    # Store information into a graph\n",
        "    graph.add_graph_documents([graph_document])"
      ],
      "metadata": {
        "id": "Gu7aaanyU2NW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Read the wikipedia article\n",
        "raw_documents = WikipediaLoader(query=\"Walt Disney\").load()\n",
        "# Define chunking strategy\n",
        "text_splitter = TokenTextSplitter(chunk_size=384, chunk_overlap=0)\n",
        "\n",
        "documents = text_splitter.split_documents(raw_documents[:3])"
      ],
      "metadata": {
        "id": "wCvmpkhnyL0R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
        "    extract_and_store_graph(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CULLN4VMU_Ou",
        "outputId": "83858428-8c8f-432e-9dc6-583b49a32e34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [04:25<00:00, 29.51s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the graph\n",
        "graph.query(\"MATCH (n) DETACH DELETE n\")"
      ],
      "metadata": {
        "id": "5wIIu5DJQVKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify which node labels should be extracted by the LLM\n",
        "allowed_nodes = [\"Person\", \"Company\", \"Location\", \"Event\", \"Movie\", \"Service\", \"Award\"]\n",
        "\n",
        "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
        "    extract_and_store_graph(d, allowed_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shS8aPGAvZgT",
        "outputId": "8e2a71c6-de2b-43f2-ef31-194d24bf150f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [04:39<00:00, 31.02s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the knowledge graph in a RAG application\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "\n",
        "graph.refresh_schema()\n",
        "\n",
        "cypher_chain = GraphCypherQAChain.from_llm(\n",
        "    graph=graph,\n",
        "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
        "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
        "    validate_cypher=True, # Validate relationship directions\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "8EeZG9I8JYGJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cypher_chain.run(\"When was Walter Elias Disney born?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "0-qaxjC2NOJm",
        "outputId": "d4dc77fe-2dd3-462a-8bf9-0dbad0820e59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (p:Person {name: \"Walter Elias Disney\"}) RETURN p.birthdate\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'p.birthdate': '1901-12-05'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Walter Elias Disney was born on December 5, 1901.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnBM5jSDNTP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}